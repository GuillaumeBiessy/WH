---
title: "A Modern Take on Whittaker-Henderson Smoothing"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{A Modern Take on Whittaker-Henderson Smoothing}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: biblio.bib  
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "",
  fig.width = 10, 
  fig.asp = 0.618,
  fig.align = "center",
  out.width = "100%"
)

library(WH)
```

## What is Whittaker-Henderson smoothing ?

### Origin

Whittaker-Henderson (WH) smoothing is a gradation method aimed at correcting the effect of sampling fluctuations on an observation vector. It is applied to evenly-spaced discrete observations. Initially proposed by @whittaker1922new for constructing mortality tables and further developed by the works of @henderson1924new, it remains one of the most popular methods among actuaries for constructing experience tables in life insurance. Extending to two-dimensional tables, it can be used for studying various risks, including but not limited to: mortality, disability, long-term care, lapse, mortgage default, and unemployment.

### The one-dimensional case

Let $\mathbf{y}$ be a vector of observations and $\mathbf{w}$ a vector of positive weights, both of size $n$. The estimator associated with Whittaker-Henderson smoothing is given by:

$$
\hat{\mathbf{y}} = \underset{\boldsymbol{\theta}}{\text{argmin}}\{F(\mathbf{y},\mathbf{w},\boldsymbol{\theta}) + R_{\lambda,q}(\boldsymbol{\theta})\}
$$

where:

-   $F(\mathbf{y},\mathbf{w},\boldsymbol{\theta}) = \underset{i = 1}{\overset{n}{\sum}} w_i(y_i - \theta_i)^2$ represents a fidelity criterion to the observations,

-   $R_{\lambda,q}(\boldsymbol{\theta}) = \lambda \underset{i = 1}{\overset{n - q}{\sum}} (\Delta^q\boldsymbol{\theta})_i^2$ represents a smoothness criterion.

In the latter expression, $\Delta^q$ denotes the forward difference operator of order $q$, such that for any $i\in[1,n - q]$:

$$
(\Delta^q\boldsymbol{\theta})_i = \underset{k = 0}{\overset{q}{\sum}} \begin{pmatrix}q \\ k\end{pmatrix}(- 1)^{q - k} \theta_{i + k}.
$$

Let us define $W = \text{Diag}(\mathbf{w})$, the diagonal matrix of weights, and $D_{n,q}$ as the order $q$ difference matrix of dimensions $(n-q) \times n$, such that $(D_{n,q}\boldsymbol{\theta})_i = (\Delta^q\boldsymbol{\theta})_i$ for all $i \in [1, n-q]$. The most commonly used difference matrices of order 1 and 2 have the following forms:

$$
D_{n,1} = \begin{bmatrix}
-1 & 1 & 0 & \ldots & 0 \\
0 & -1 & 1 & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & \ldots & 0 & -1 & 1 \\
\end{bmatrix}
\quad\text{and}\quad
D_{n,2} = \begin{bmatrix}
1 & -2 & 1 & 0 & \ldots & 0 \\
0 & 1 & -2 & 1 & \ldots & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
0 & \ldots & 0 & 1 & -2 & 1 \\
\end{bmatrix}.
$$

The fidelity and smoothness criteria can be rewritten in matrix form as:

$$
F(\mathbf{y},\mathbf{w},\boldsymbol{\theta}) = (\mathbf{y} - \boldsymbol{\theta})^TW(\mathbf{y} - \boldsymbol{\theta}) \quad \text{and} \quad R_{\lambda,q}(\boldsymbol{\theta}) = \lambda\boldsymbol{\theta}^TD_{n,q}^TD_{n,q}\boldsymbol{\theta}.
$$

The associated estimator for smoothing becomes:

$$
\hat{\mathbf{y}} = \underset{\boldsymbol{\theta}}{\text{argmin}} \left\lbrace(\mathbf{y} - \boldsymbol{\theta})^TW(\mathbf{y} - \boldsymbol{\theta}) + \boldsymbol{\theta}^TP_{\lambda}\boldsymbol{\theta}\right\rbrace
$$

where $P_{\lambda} = \lambda D_{n,q}^TD_{n,q}$.

### The two-dimensional case

In the bidimensional case, let us consider a matrix $Y$ of observations and a matrix $\Omega$ of non-negative weights, both of dimensions $n_x \times n_z$. The estimator associated with the Whittaker-Henderson smoothing can be written as:

$$
\widehat{Y} = \underset{\Theta}{\text{argmin}}\{F(Y,\Omega, \Theta) + R_{\lambda,q}(\Theta)\}
$$

where:

- $F(Y,\Omega, \Theta) = \sum_{i = 1}^{n_x}\sum_{j = 1}^{n_z} \Omega_{i,j}(Y_{i,j} - \Theta_{i,j})^2$ represents a fidelity criterion to the observations,

- $R_{\lambda,q}(\Theta) = \lambda_x \sum_{j = 1}^{n_z}\sum_{i = 1}^{n_x - q_x} (\Delta^{q_x}\Theta_{\bullet,j})_i^2 + \lambda_z \sum_{i = 1}^{n_x}\sum_{j = 1}^{n_z - q_z} (\Delta^{q_z}\Theta_{i,\bullet})_j^2$ is a smoothness criterion.

This latter criterion can be written as the sum of two one-dimensional regularization criteria, with orders $q_x$ and $q_z$, applied respectively to all rows and all columns of $\Theta$. It is also possible to adopt matrix notations by defining $\mathbf{y} = \textbf{vec}(Y)$, $\mathbf{w} = \textbf{vec}(\Omega)$, and $\boldsymbol{\theta} = \textbf{vec}(\Theta)$ as the vectors obtained by stacking the columns of the matrices $Y$, $\Omega$, and $\Theta$, respectively. Additionally, let us denote $W = \text{Diag}(\mathbf{w})$ and $n = n_x \times n_z$. The fidelity and smoothness criteria can then be rewritten as linear combinations of the vectors $\mathbf{y}$, $\mathbf{w}$, and $\boldsymbol{\theta}$:

$$
\begin{aligned}
F(\mathbf{y},\mathbf{w}, \boldsymbol{\theta}) &= (\mathbf{y} - \boldsymbol{\theta})^TW(\mathbf{y} - \boldsymbol{\theta}) \\
R_{\lambda,q}(\boldsymbol{\theta}) &= \boldsymbol{\theta}^{T}(\lambda_x I_{n_z} \otimes D_{n_x,q_x}^{T}D_{n_x,q_x} + \lambda_z D_{n_z,q_z}^{T}D_{n_z,q_z} \otimes I_{n_x}) \boldsymbol{\theta}.
\end{aligned}
$$

and the estimator associated with the smoothing takes the same form as in the one-dimensional case:

$$
\hat{\mathbf{y}} = \underset{\boldsymbol{\theta}}{\text{argmin}} \left\lbrace(\mathbf{y} - \boldsymbol{\theta})^TW(\mathbf{y} - \boldsymbol{\theta}) + \boldsymbol{\theta}^TP_{\lambda}\boldsymbol{\theta}\right\rbrace
$$

except in this case $P_{\lambda} = \lambda_x I_{n_z} \otimes D_{n_x,q_x}^{T}D_{n_x,q_x} + \lambda_z D_{n_z,q_z}^{T}D_{n_z,q_z} \otimes I_{n_x}$.

### Explicit solution

Whittaker-Henderson estimator has an explicit solution:

$$\hat{\mathbf{y}} = (W + P_{\lambda})^{-1}W\mathbf{y}.$$

Indeed, as a minimum, $\hat{\mathbf{y}}$ satisfies:

$$0 = \left.\frac{\partial}{\partial \boldsymbol{\theta}}\right|_{\hat{\mathbf{y}}}\left\lbrace(\mathbf{y} - \boldsymbol{\theta})^{T}W(\mathbf{y} - \boldsymbol{\theta}) + \boldsymbol{\theta}^{T}P_{\lambda}\boldsymbol{\theta}\right\rbrace = - 2 X^TW(y - \hat{\mathbf{y}}) +2X^TP_{\lambda} \hat{\mathbf{y}}.$$

It follows that $(W + P_{\lambda})\hat{\boldsymbol{\theta}} = W\mathbf{y}$, and if $W + P_{\lambda}$ is invertible, then $\hat{\mathbf{y}}$ is indeed a solution for the original equation. When $\lambda \neq 0$, $W + P_{\lambda}$ is invertible as long as $\mathbf{w}$ has $q$ non-zero elements in the one-dimensional case, and $\Omega$ has at least $q_x \times q_z$ non-zero elements distributed over $q_x$ different rows and $q_z$ different columns in the two-dimensional case. These are sufficient conditions, which are always satisfied in practice and will not be demonstrated here.

## How to use the package ?

The `WH` package features two main functions `WH_1d` and `WH_2d` corresponding to the one-dimensional and two-dimensional cases respectively. Two arguments are mandatory for those functions:

* The vector (or matrix in the two-dimension case) `d` corresponding to the number of observed events of interest by age (or by age and duration in the two-dimension case). `d` should have named elements (or rows and columns) for the model results to be extrapolated.

* The vector (or matrix in the two-dimension case) `ec` corresponding to the portfolio central exposure by age (or by age and duration in the two-dimension case) whose dimensions should match those of `d`. The contribution of each individual to the portfolio central exposure corresponds to the time the individual was actually observed with corresponding age (and duration in the two-dimension cas). It always ranges from 0 to 1 and is affected by individuals leaving the portfolio, no matter the cause, as well as censoring and truncating phenomena.

Additional arguments may be supplied, whose description is given in the documentation of the functions.

The package also embed two fictive agregated datasets to illustrate how to use it:

* `portfolio_mortality` contains the agregated number of deaths and associated central exposure by age for an annuity portfolio.

* `portfolio_LTC` contains the agregated number of deaths and associated central exposure by age and duration (in years) since the onset of LTC for the annuitant database of a long-term care portfolio.

```{r fit-1d}
# One-dimension case
keep <- which(portfolio_mort$ec > 0) # observations with no data
d <- portfolio_mort$d[keep]
ec <- portfolio_mort$ec[keep]

WH_1d_fit <- WH_1d(d, ec)
```

```{r fit-2d}
# Two_dimension case
keep_age <- which(rowSums(portfolio_LTC$ec) > 1e2)
keep_duration <- which(colSums(portfolio_LTC$ec) > 1e2)

d  <- portfolio_LTC$d[keep_age, keep_duration]
ec <- portfolio_LTC$ec[keep_age, keep_duration]

WH_2d_fit <- WH_2d(d, ec)
```

Functions `WH_1d` and `WH_2d` output objects of class `"WH_1d"` and `"WH_2d"` to which additional functions (including generic S3 methods) may be applied:

* The `print` function provides a glimpse of the fitted results

```{r print}
WH_1d_fit
WH_2d_fit
```

* The `plot` function generates rough plots of the model fit, the associated standard deviation, the model residuals or the associated degrees of freedom. See the `plot.WH_1d` and `plot.WH_2d` functions help for more details.

```{r plot}
plot(WH_1d_fit)
plot(WH_1d_fit, "res")
plot(WH_1d_fit, "edf")

plot(WH_2d_fit)
plot(WH_2d_fit, "std_y_hat")
```

* The `predict` function generates an extrapolation of the model. It requires a `newdata` argument, a named list with one or two elements corresponding to the positions of the new observations. In the two-dimension case constraints are used so that the predicted values matches the fitted values for the initial observations [see @carballo2021prediction to understand why this is required].

```{r predict}
WH_1d_fit |> predict(newdata = 18:99) |> plot()
WH_2d_fit |> predict(newdata = list(age = 50:99,
                                    duration = 0:19)) |> plot()
```


* Finally the `output_to_df` function converts an `"WH_1d"` or `"WH_2d"` object into a `data.frame`. Information about the fit is discarded in the process. This function may be useful to produce better visualizations from the data, for example using the ggplot2 package.

```{r}
WH_1d_df <- WH_1d_fit |> output_to_df()
WH_2d_df <- WH_2d_fit |> output_to_df()
```

## Further WH smoothing theory

### Role of the penalization

In the smoothing equation, $(y - \theta)^{T}W(y - \theta)$ is a fidelity criterion $\theta^{T}P_\lambda\theta$ a regularity criterion. The relative importance of those criterions is controlled by the smoothing parameter (or parameter vectors in the two-dimension case) $\lambda$. 

In the one-dimension case, the penalization matrix may be rewritten:

$$
\theta^{T}P_\lambda\theta = \begin{cases}\lambda\underset{i = 1}{\overset{n - 1}{\sum}}(\theta_{i + 1} - \theta_i)^2 & \text{si }q = 1\\ \lambda\underset{i = 1}{\overset{n - 2}{\sum}}([\theta_{i + 2} - \theta_{i + 1}] - [\theta_{i + 1} - \theta_i])^2 & \text{si }q = 2
\end{cases}
$$

It is easily seen that:

* In the case where $q_x = 1$ then this is equal to 0 only in the case where, for all $i\in[1,n_x - 1]$, $\theta_{i + 1} - \theta_i = 0$ or equivalently for all $i\in[1,n_x]$, $\theta_i = \theta_1$. 

* In the case where $q_x = 2$, this is equal to 0 only in the case where, for all $i\in[1,n_x - 2]$, $\theta_{i + 2} - \theta_{i + 1} = \theta_{i + 1} - \theta_i$, or equivalently for all $i\in[1,n_x]$, $\theta_i = \theta_1 + (i - 1)(\theta_2 - \theta_1)$. 

In the sense of the penalization of order $q_x$, the space of observation vectors that are totally smooth is therefore the space of polynomial functions of degrees at most $q - 1$. Those results carry on to two-dimension smoothing where a totally smooth observation matrix $Y$ corresponds to constant / aligned observations on the rows / columns of the observation matrix, depending on the values of $q_x$ and $q_z$.

### Credibility intervals

From the explicit solution of WH smoothinh, $\mathbb{E}(\hat{\mathbf{y}}) = (W + P_{\lambda})^{-1}W\mathbb{E}(\mathbf{y}) \ne \mathbb{E}(\mathbf{y})$ when $\lambda \ne 0$. This implies that penalization introduces a smoothing bias, which prevents the construction of a confidence interval centered around $\mathbb{E}(\mathbf{y})$. Therefore, in this section, we turn to a Bayesian approach where smoothing can be interpreted more naturally.

Let us suppose that $\mathbf{y} | \boldsymbol{\theta}\sim \mathcal{N}(\boldsymbol{\theta}, W^{-})$ and $\boldsymbol{\theta} \sim \mathcal{N}(0, P_{\lambda}^{-})$ where $P_{\lambda}^{-}$ denotes the pseudo-inverse of the matrix $P_{\lambda}$. The Bayes' formula allows us to express the posterior likelihood $f(\boldsymbol{\theta} | \mathbf{y})$ associated with these choices in the following form:

$$\begin{aligned}
f(\boldsymbol{\theta} | \mathbf{y}) &= \frac{f(\mathbf{y} | \boldsymbol{\theta}) f(\boldsymbol{\theta})}{f(y)} \\
&\propto f(\mathbf{y} | \boldsymbol{\theta}) f(\boldsymbol{\theta}) \\
&\propto \exp\left(- \frac{1}{2}(\mathbf{y} - \boldsymbol{\theta})^{T}W(\mathbf{y} - \boldsymbol{\theta})\right)\exp\left(-\frac{1}{2}\boldsymbol{\theta}^TP_{\lambda} \boldsymbol{\theta}\right) \\
&\propto \exp\left(- \frac{1}{2}\left[(\mathbf{y} - \boldsymbol{\theta})^{T}W(\mathbf{y} - \boldsymbol{\theta}) + \boldsymbol{\theta}^TP_{\lambda}\boldsymbol{\theta}\right]\right).
\end{aligned}$$

The mode of the posterior distribution, $\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta}}{\text{argmax}} [f(\boldsymbol{\theta} | \mathbf{y})]$, also known as the maximum a posteriori (MAP) estimate, coincides with the explicit solution $\hat{\mathbf{y}}$. 

A second-order Taylor expansion of the log-posterior likelihood around $\hat{\mathbf{y}} = \hat{\boldsymbol{\theta}}$ gives us:

$$\ln f(\boldsymbol{\theta} | \mathbf{y}) = \ln f(\hat{\boldsymbol{\theta}} | \mathbf{y}) + \left.\frac{\partial \ln f(\boldsymbol{\theta} | \mathbf{y})}{\partial \boldsymbol{\theta}}\right|_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}}^{T}(\boldsymbol{\theta} - \hat{\boldsymbol{\theta}}) + \frac{1}{2}(\boldsymbol{\theta} - \hat{\boldsymbol{\theta}})^{T} \left.\frac{\partial^2 \ln f(\boldsymbol{\theta} | \mathbf{y})}{\partial \boldsymbol{\theta} \partial \boldsymbol{\theta}^{T}}\right|_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}}(\boldsymbol{\theta} - \hat{\boldsymbol{\theta}})$${#eq-taylor1}

$$\text{where} \quad \left.\frac{\partial \ln f(\boldsymbol{\theta} | \mathbf{y})}{\partial \boldsymbol{\theta}}\right|_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}} = 0 \quad \text{and} \quad \left.\frac{\partial^2 \ln f(\boldsymbol{\theta} | \mathbf{y})}{\partial \boldsymbol{\theta} \partial \boldsymbol{\theta}^{T}}\right|_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}} = - (W + P_{\lambda}).$$

Note that this last derivative no longer depends on $\boldsymbol{\theta}$, and higher-order derivatives beyond 2 are all zero. The Taylor expansion allows for an exact computation of $\ln f(\boldsymbol{\theta} | \mathbf{y})$. By substituting the result back into the Taylor expansion, we obtain:

$$\begin{aligned}
f(\boldsymbol{\theta} | \mathbf{y}) &\propto \exp\left[\ln f(\hat{\boldsymbol{\theta}} | \mathbf{y}) - \frac{1}{2} (\boldsymbol{\theta} - \hat{\boldsymbol{\theta}})^{T}(W + P_{\lambda})(\boldsymbol{\theta} - \hat{\boldsymbol{\theta}})\right] \\
&\propto \exp\left[- \frac{1}{2} (\boldsymbol{\theta} - \hat{\boldsymbol{\theta}})^{T}(W + P_{\lambda})(\boldsymbol{\theta} - \hat{\boldsymbol{\theta}})\right]
\end{aligned}$$

which can immediately be recognized as the density of the $\mathcal{N}(\hat{\boldsymbol{\theta}},(W + P_{\lambda})^{- 1})$ distribution.

The assumption $\boldsymbol{\theta} \sim \mathcal{N}(0, P_{\lambda}^{-})$ corresponds to a simple Bayesian formalization of the smoothness criterion. It reflects a (improper) prior belief of the modeler about the underlying distribution of the observation vector $\mathbf{y}$.

The use of Whittaker-Henderson smoothing in the Bayesian framework and the construction of credible intervals are conditioned on the validity of the assumption $\mathbf{y} | \boldsymbol{\theta}\sim \mathcal{N}(\boldsymbol{\theta}, W^{-})$. The components of the observation vector should be independent and have known variances. The weight vector $\mathbf{w}$ used should contain the inverses of these variances. Under these assumptions, credible intervals at $100(1 - \alpha)$% for smoothing can be constructed and take the form:

$$\mathbb{E}(\mathbf{y}) | \mathbf{y} \in \left[\hat{\mathbf{y}} \pm \Phi\left(1 -\frac{\alpha}{2}\right)\sqrt{\textbf{diag}\left\lbrace(W + P_{\lambda})^{-1}\right\rbrace}\right]$$

with probability $1 -\frac{\alpha}{2}$ where $\hat{\mathbf{y}} = (W + P_{\lambda})^{- 1}W\mathbf{y}$ and $\Phi$ denotes the cumulative distribution function of the standard normal distribution. According to @marra2012, these credible intervals provide satisfactory coverage of the corresponding frequentist confidence intervals and can therefore be used as practical substitutes.

## What should I use as observations and weights ?

The previous section show that applying WH smoothing to a couple $(y,w)$ such that $y \sim \mathcal{N}(\theta, \sigma^2W^{- 1})$ where $\theta$ is the underlying law we want to estimate, $\sigma^2$ is an overdispersion parameter to be estimated, and $W = \text{Diag}(w)$, then, using a bayesian interpration, credibility intervals were available for the posterior distribution of $\theta | y$. In this section, in the framework of survival analysis models, we exhibit a candidate for $(y,w)$.

### Survival analysis framework - one-dimension case

Let us consider the observation of $m$ individuals in the cas of a longitudinal study under left-truncating and right-censoring phenomena. Let us assume a single law must be estimated (for example a mortality law) and that it depends on a single covariate named $x$ (for example the age of the insured). This law may be entirely characterized by providing one of the 3 following quantities:

-   The cumulative distribution function $F(x)$ or the survival function $S(x) = 1 - F(x)$,

-   The associated density function $f(x) = - \frac{\text{d}}{\text{d}x}S(x)$,

-   The hazard rate $\mu(x) = - \frac{\text{d}}{\text{d}x}\text{ln} S(x)$

Let us assume that the underlying law depends on a parameter vector $\beta$ that is to be estimated using maximum likelihood. The likelihood associated with the observation of the individuals is:

$$
\mathcal{L}(\beta) = \underset{i = 1}{\overset{m}{\prod}} \left[\frac{f(x_i + t_i,\beta)}{S(x_i,\beta)}\right]^{\delta_i}\left[\frac{S(x_i + t_i,\beta)}{S(x_i,\beta)}\right]^{1 - \delta_i}
$$

where for each $x_i$ represents the age at the start of the observation, $t_i$ is the observation duration and $\delta_i$ is 1 if the event of interest (for example death) has been observed and 0 otherwise. Those 3 elements should be computed by taking into account the date of subscribing and a possible termination date (for example beauce of policy lapse) for each individual, possible presence of a waiting period and exclusion of specific periods of time because of incomplete data or medical underwriting effects. The observation period may therefore be shorter than the actual period of presence of the individuals in the portfolio.

Maximization of the previous likelihood may be rewritten by taking the logarithm and using the relations:

$$\begin{aligned}
S(x) & = \exp\left(\underset{u = 0}{\overset{x}{\int}}\mu(u)\text{d}u\right) & f(x) & = \mu(x)S(x)
\end{aligned}$$

which yields, after a few simplifications:

$$
\ell(\beta) = \underset{i = 1}{\overset{m}{\sum}} \left[\delta_i \ln\mu(x_i + t_i,\beta) - \underset{u = 0}{\overset{t_i}{\int}}\mu(x_i + u,\beta)\text{d}u\right]
$$

Let us assume that the hazard rate is a piecewise constant function on one-year interval between integer ages or more formally: $\mu(x + \epsilon) = \mu(x)$ for all $x \in \mathbb{N}$ and $\epsilon \in [0,1[$.

Let us further note that if $\mathbf{1}$ is the index function, then for all $0 \le a < x_{\max}$, $\underset{x = x_{\min}}{\overset{x_{\max}}{\sum}} \mathbf{1}(x \le a < x + 1) = 1$ by noting $x_{\min} = \min(x)$ and $x_{\max} = \max(x)$. The previous likelihood therefore becomes:

$$\ell(\beta) = \underset{i = 1}{\overset{m}{\sum}} \left[\underset{x = x_{\min}}{\overset{x_{\max}}{\sum}} \delta_i\mathbf{1}(x \le x_i + t_i < x + 1)  \ln\mu(x_i + t_i,\beta) - \underset{u = 0}{\overset{t_i}{\int}}\underset{x = x_{\min}}{\overset{x_{\max}}{\sum}} \mathbf{1}(x \le x_i + u < x + 1)\mu(x_i + u,\beta)\text{d}u\right]$$

The piecewise constant assumption yields $\mathbf{1}(x \le x_i + t_i < x + 1) \ln\mu(x_i + t_i,\beta) = \mathbf{1}(x \le x_i + t_i < x + 1) \ln\mu(x,\beta)$ and $\mathbf{1}(x \le x_i + u < x + 1)\mu(x_i + u,\beta) = \mathbf{1}(x \le x_i + u < x + 1) \ln\mu(x,\beta)$. The two sums may then be interverted, giving:

$$\begin{aligned}
\ell(\beta) &= \underset{x = x_{\min}}{\overset{x_{\max}}{\sum}} \left[\ln\mu(x,\beta) d(x) - \mu(x,\beta) e_c(x)\right] \quad \text{where} \\
d(x) & = \underset{i = 1}{\overset{m}{\sum}} \delta_i \mathbf{1}(x \le x_i + t_i < x + 1)  \quad \text{and} \\
e_c(x) & = \underset{i = 1}{\overset{m}{\sum}}\underset{u = 0}{\overset{t_i}{\int}}\mathbf{1}(x \le x_i + u < x + 1)\text{d}u = \underset{i = 1}{\overset{m}{\sum}} \left[\min(t_i, x - x_i + 1) - \max(0, x - x_i)\right]^+
\end{aligned}$$

where $d(x)$ and $e_c(x)$ corresponds respectively to the number of observed deaths between age $x$ and $x + 1$ and to the sum of the observation duration between those dates, by noting $a^+ = \max(a, 0)$.

### Extension to the two-dimension case

The extension of the previous approach to the two-dimension case only requires minor adjustements to the previous proposition. Let us note $z_{\min} = \min(z)$ and $z_\text{max} = \max(z)$. The piecewise constant assumption needs to be extended to each of the two dimensions. Formally we now assume that $\mu(x + \epsilon, z + \xi) = \mu(x, z)$ for all $x, z \in \mathbb{N}$ and $\epsilon, \xi \in [0,1[$. The sums on $x$ are replaced by double sums on the values of both $x$ and $z$ and the likelihood becomes:

$$\begin{aligned}
\ell(\beta) &= \underset{x = x_{\min}}{\overset{x_{\max}}{\sum}} \underset{z = z_{\min}}{\overset{z_{\max}}{\sum}}\left[\ln\mu(x,z,\beta) d(x,z) - \mu(x,z,\beta) e_c(x,z)\right] \quad \text{where} \\
d(x,z) & = \underset{i = 1}{\overset{m}{\sum}} \delta_i \mathbf{1}(x \le x_i + t_i < x + 1) \mathbf{1}(z \le z_i + t_i < z + 1)  \quad \text{and}\\
e_c(x,z) & = \underset{i = 1}{\overset{m}{\sum}}\underset{u = 0}{\overset{t_i}{\int}}\mathbf{1}(x \le x_i + u < x + 1)\mathbf{1}(z \le z_i + u < z + 1)\text{d}u \\
& = \underset{i = 1}{\overset{m}{\sum}} \left[\min(t_i, x + 1 - x_i, z + 1 - z_i) - \max(0, x - x_i, z - z_i)\right]^+.
\end{aligned}$$

### Likelihood equations

The log-likelihood in the one-dimension or two-dimension cases may be expressed on the common vectorial form $\ell(\beta) = \ln\mu(\beta)^{T}d - \mu(\beta)^{T}e_c$ where $d$ and $e_c$ corresponds respectively to the vector of expected events and associated central exposure.

In the particular case of log-linear models, the hazard rate may be defined as $\ln\mu(\beta) = X\beta$ with $X$ a matrix of dimensions $n \times p$ and full-rank $p \le n$. The use of the logarithmic link ensures that $\mu = \exp(X\beta)$ is always positive. Derivatives of the log-likelihood function are for this model:

$$\frac{\partial \ell}{\partial \beta} = X^{T}\left[d - \exp(X\beta) \odot e_c\right] \quad \text{and} \quad \frac{\partial^2 \ell}{\partial\beta^2} = - X^{T}W_{\beta}X \quad \text{where} \quad W_{\beta} = \text{Diag}(\exp(X\beta) \odot e_c).$$
Let us note that those likelihood equations are exactly those that would have been obtained by treating the central exposure as a deterministic quantity and by assuming that the number of deaths follows a Poisson GLM of parameter $\mu(\beta)\times e_c$. The model above therefore behave as a Poisson GLM [@nelder1972glm].

### Consequences for WH smoothing

Whittaker-Henderson focuses on the particular case where $X = I_n$ and $\beta = \theta$. In that case the previous equations yield an explicit solution $\beta = \ln(d) - \ln(e_c)$ and thus $\mu = d / e_c$.

Using the asymptotical properties of the maximum likelihood estimator, we obtain $\hat{\beta} \sim \mathcal{N}(\beta, W_{\hat{\beta}}\,^{- 1})$ where diagonal elements of $W_{\hat{\beta}}$ are simply $e_c \exp(\hat{\beta}) = e_c d / e_c = d$.

Thus it has been shown that in the survival analysis framework introduced, asymptotically $\ln(d / e_c) \sim \mathcal{N}(\ln(\mu), W^{- 1})$ where $W = \text{Diag}(d)$. This justify applying WH smoothing to the observation vector $y = \ln(d / e_c)$ with weights $w = d$. The overdispersion parameter is in this cas simply $\sigma^2 = 1$. We obtain credibility intervals for the posterior distribution $\theta | y$ of the form: $[\hat{y} \pm \Phi(\frac{1 - \alpha}{2})\sqrt{\text{diag}\left\lbrace(W + P_\lambda)^{- 1}\right\rbrace}]$.

### Generalization to penalized maximum likelihood

The previous approach relies on the asymptotic properties of the maximum likelihood estimator. When few data is available, those properties may not hold. An alternative approach is to apply the penalization from the WH smoothing directly to the previous likelihood function and thus maximize the penalizaed likelihood $\ell_P(\beta) = \ell(\beta) - \beta^{T}P_\lambda\beta / 2$. This can be seen as a generalization of WH smoothing to non-gaussian likelihood. Still assuming a log-linear model is used, derivatives of the log-likelihood functions become:

$$\frac{\partial \ell_P}{\partial \beta} = X^{T}\left[d - \exp(X\beta) \odot e_c\right] - P_\lambda\beta \quad \text{and} \quad \frac{\partial^2 \ell_P}{\partial\beta^2} = - (X^{T}W_{\beta}X + P_\lambda) \quad \text{where} \quad W_{\beta} = \text{Diag}(\exp(X\beta) \odot e_c).$$

Unlike the previously encountered likelihood, those equations does not have an explicit solution, even when $X = I_n$, because both $X\beta$ and $\exp(X\beta)$ appear in the equations. Using Newton algorithm, a series of estimators $(\beta_k)_{k \ge 0}$ may be built so that it converges to the penalized maximum likelihood estimator $\hat{\beta} = \underset{\beta}{\text{argmin}}\:l_P(\beta)$. 

Those estimators are defined by:

$$
\begin{aligned}
\beta_{k + 1} &= \beta_k - \left(\left.\frac{\partial^2 \ell_P}{\partial\beta^2}\right|_{\beta = \beta_k}\right)^{- 1} \left.\frac{\partial \ell_P}{\partial\beta}\right|_{\beta = \beta_k} \\ 
&= \beta_k + (X^{T}W_kX + P_\lambda)^{- 1} \left[X^{T}\left(d - \exp(X\beta_k) \odot e_c\right) - P_\lambda \beta_k\right] \\ 
&= \Psi_k X^{T}W_k z_k
\end{aligned}
$$

by noting $\eta_k = X\beta_k$, $\Psi_k = (X^{T}W_kX + P_\lambda)^{- 1}$, $W_k = \text{Diag}(\exp(\eta_k) \odot e_c)$ and $z_k = \eta_k + W_k^{- 1}[d - \exp(\eta_k) \odot e_c]$. Setting $\eta_0 = \ln d - \ln e_c$ yields an adequate starting point to the algorithm (the starting $\beta_0$ does not have to be provided). This implies that $W_0 = \text{Diag}(d)$ and $\mathbf{z}_0 = \ln d - \ln e_c$ which corresponds to the observations and weights used in the regression framework. The update of $\beta_k$ in the Newton optimization step boils down to applying WH smoothing to the couple ($z_k$, $W_k$). The first estimator $\beta_1$ computed is therefore the solution of WH smoothing in the regression framework. Further iterations relies on the *working vector* $z_k$ and associated weights $W_k$ that are adjusted at each step based on the previous step results.

The posterior distribution of $\beta | y$ may be asymptotically approached by $\mathcal{N}(\hat{\beta}, (X^TW_{\hat{\beta}}X + P_\lambda)^{- 1})$, which allows $100(1 -\alpha)\%$ credibility intervals of the form $\left[\hat{\beta} \pm \Phi(1 - \alpha / 2) \sqrt{\text{diag}\lbrace(X^TW_{\hat{\beta}}X + P_\lambda)^{- 1}\rbrace}\right]$ where $\Phi$ is the cumulative distribution function of the normal distribution. 

## How is the optimal smoothing parameter determined ?

In the definition of WH smoothing, $(\mathbf{y} - \boldsymbol{\theta})^{T}W(\mathbf{y} - \boldsymbol{\theta})$ represents a fidelity criterion to the observations, and $\boldsymbol{\theta}^{T}P_{\lambda}\boldsymbol{\theta}$ represents a smoothness criterion. The relative importance of these criteria is controlled by the parameter (or pair of parameters in the two-dimensional case) $\lambda$. The result of smoothing is highly sensitive to the chosen value of the smoothing parameter.

# References {-}
